{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing library\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "## Read file\n",
    "file_name = '<file_name>'\n",
    "## Read file using pandas\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix(typeToLoad, vocab_dict):\n",
    "    import gensim.models.keyedvectors as word2vec\n",
    "    import gc\n",
    "\n",
    "    # load different embedding file from Kaggle depending on which embedding\n",
    "    # matrix we are going to experiment with\n",
    "    if (typeToLoad == \"gloveTwitter50d\"):\n",
    "        EMBEDDING_FILE = 'embeddings\\glove-twitter-27b-50d/glove.twitter.27B.50d.txt'\n",
    "        embed_size = 50\n",
    "    elif (typeToLoad == \"word2vec\"):\n",
    "        word2vecDict = word2vec.KeyedVectors.load_word2vec_format( \"embeddings\\GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "        embed_size = 300\n",
    "    elif (typeToLoad == \"fasttext\"):\n",
    "        EMBEDDING_FILE = 'embeddings\\\\fasttext/wiki.simple.vec'\n",
    "        embed_size = 300\n",
    "    elif (typeToLoad == \"glove840B300D\"):\n",
    "        EMBEDDING_FILE = 'embeddings\\glove.840B.300d/glove.840B.300d.txt'\n",
    "        embed_size = 300\n",
    "    elif (typeToLoad == \"glove6B300D\"):\n",
    "        EMBEDDING_FILE = 'embeddings\\glove.6B\\glove.6B.300d.txt'\n",
    "        embed_size = 300\n",
    "    elif (typeToLoad == \"paragram\"):\n",
    "        EMBEDDING_FILE = 'embeddings\\paragram_300_sl999\\paragram_300_sl999.txt'\n",
    "        embed_size = 300\n",
    "    elif (typeToLoad == \"wikiNews\"):\n",
    "        EMBEDDING_FILE = \"embeddings\\wiki-news-300d-1M\\wiki-news-300d-1M.vec\"\n",
    "        embed_size = 300\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    if (typeToLoad in [\"gloveTwitter50d\", \"fasttext\"]):\n",
    "        embeddings_index = dict()\n",
    "        # Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
    "        f = open(EMBEDDING_FILE)\n",
    "        for line in f:\n",
    "            # split up line into an indexed array\n",
    "            values = line.rstrip().rsplit(' ')  # line.split()\n",
    "            # first index is word\n",
    "            word = values[0]\n",
    "            # store the rest of the values in the array as a new array\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs  # 50 dimensions\n",
    "        f.close()\n",
    "    elif (typeToLoad in [\"glove840B300D\", \"paragram\", \"glove6B300D\"]):\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='latin'))\n",
    "    elif (typeToLoad in [\"wikiNews\"]):\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o) > 100)\n",
    "    else:\n",
    "        embeddings_index = dict()\n",
    "        for word in word2vecDict.wv.vocab:\n",
    "            embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    gc.collect()\n",
    "    # We get the mean and standard deviation of the embedding weights so that we could maintain the\n",
    "    # same statistics for the rest of our own random generated weights.\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    nb_words = len(vocab_dict)\n",
    "    # We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "    # the size will be Number of Words in Vocab X Embedding Size\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    gc.collect()\n",
    "\n",
    "    # With the newly created embedding matrix, we'll fill it up with the words that we have in both\n",
    "    # our own dictionary and loaded pretrained embedding.\n",
    "    embeddedCount = 0\n",
    "    for word, i in vocab_dict.items():\n",
    "        #i -= 1\n",
    "        # then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        # and store inside the embedding matrix that we will train later on.\n",
    "        if embedding_vector is not None:\n",
    "            try :\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                embeddedCount += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "    print('total embedded:', embeddedCount, 'common words')\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "\n",
    "    # finally, return the embedding matrix\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing sentence into token for finding synonym.\n",
    "def make_tokenizer(texts):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(texts)\n",
    "    return t\n",
    "\n",
    "tokenizer = make_tokenizer(df['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary of word index\n",
    "index_word = {}\n",
    "for word in tokenizer.word_index.keys():\n",
    "    index_word[tokenizer.word_index[word]] = word\n",
    "\n",
    "vocab_dict = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading word embedding\n",
    "from time import time\n",
    "start = time()\n",
    "embed_mat = loadEmbeddingMatrix(\"glove840B300D\", vocab_dict)\n",
    "end = time()\n",
    "print(\"Embedding loaded in \", (end-start)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "synonyms_number = 5\n",
    "word_number = 20000\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=synonyms_number+1).fit(embed_mat)\n",
    "\n",
    "neighbours_mat = nn.kneighbors(embed_mat[1:word_number])[1]\n",
    "\n",
    "synonyms = {x[0]: x[1:] for x in neighbours_mat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding nearby synonym - Basically it's not actually synonym. It's near by words of targetted word. \n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonym = {}\n",
    "for x in range(0,100):\n",
    "    try :\n",
    "        synonym.update({index_word[x] : [index_word[synonyms[x][i]] for i in range(synonyms_number-1)]})\n",
    "    except :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this synonym list to replace words with it's variation\n",
    "## Below code is in draft. But logic can be used to complete the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can only change words for selected part of speech to preserve semantic meaning.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_pos_tag (word, tagged) :\n",
    "    res = [(x, y) for x, y in tagged if x == word]\n",
    "    return res[0][1]\n",
    "\n",
    "# Load the pretrained neural net\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in df[\"Message\"]:\n",
    "    print(message)\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer.tokenize(message)\n",
    "\n",
    "    # Get the list of words from the entire text\n",
    "    words = word_tokenize(message)\n",
    "\n",
    "    # Identify the parts of speech\n",
    "    tagged = nltk.pos_tag(words, tagset=\"universal\")\n",
    "    \n",
    "    replacements = []\n",
    "\n",
    "    for word in words:\n",
    "        synonym = []\n",
    "        antonyms = []\n",
    "        word_index = vocab_dict.get(word, None)\n",
    "\n",
    "        pos_tag = get_pos_tag(word, tagged)\n",
    "        if (word_index and pos_tag in [\"ADJ\", \"ADV\", \"NOUN\", \"VERB\"] and word not in nltk.corpus.stopwords.words('english')) :\n",
    "            for syn in wordnet.synsets(word, eval(\"wordnet.\" + pos_tag)):\n",
    "                for l in syn.lemmas() :\n",
    "                    if(l.name() in [index_word[synonyms[word_index][i]] for i in range(synonyms_number-1)]):\n",
    "                        synonym.append(l.name())\n",
    "                    #if l.antonyms():\n",
    "                    #    antonyms.append(l.antonyms()[0].name())\n",
    "        \n",
    "        if (synonym) :\n",
    "            print(word, set(synonym))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
